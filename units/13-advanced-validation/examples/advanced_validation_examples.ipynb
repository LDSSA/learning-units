{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use one of the the toy datasets that scikit has available, since the focus of this example is too show how to use the validation tools and not how to deal with a \"raw\" dataset. We'll use the Boston House Prices dataset, which has a median value price for occupied home as target and 13 number of attributes, ranging from things like crime per capita rate to student to teacher ratio. The first thing we will do is to load the dataset and check its shape and values for one of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = datasets.load_boston()\n",
    "boston.data.shape, boston.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6.32000000e-03   1.80000000e+01   2.31000000e+00   0.00000000e+00\n",
      "   5.38000000e-01   6.57500000e+00   6.52000000e+01   4.09000000e+00\n",
      "   1.00000000e+00   2.96000000e+02   1.53000000e+01   3.96900000e+02\n",
      "   4.98000000e+00]\n",
      "24.0\n"
     ]
    }
   ],
   "source": [
    "print(boston.data[0])\n",
    "\n",
    "print(boston.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we may see, we have 506 samples, with a part named _data_, where we have the 13 attributes and a part named _target_, with the target prices for each of those sets of 13 attributes.\n",
    "\n",
    "We will now procede to train some regression models in order to predict the price of a house given the 13 attributes and introduce methods of validating your results in order to obtain a model able to generalize for unseen data.\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple cross-validation on a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic of simple cross-validation is to train several models (both different algorithms and with different parameters) and in the end choose the one that yields the best accuracy on a test set.\n",
    "\n",
    "We will start to tackle this problem by splitting the data set into four different arrays. **X_train** and **y_train** have the attributes and target prices for a subset of the samples we have in order to train the model, and **X_test** and **y_test** have the attributes and target prices for the rest of the subset, in order to verify the accuracy of the model. This might be done using the [train\\_test\\_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function on scikit. In this example we will use 30% of the dataset as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 13) (354,)\n",
      "(152, 13) (152,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    boston.data, boston.target, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will train models to solve this problem. There are several models that can be used for training and in the beginning is hard to have the feeling for the best ones for each task. A good way to start is by following [scikit-learn algorithm cheat-sheet](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html). As we can see, from what we know from out dataset (predicting a quantity, less than 100k samples and 13 features that intuitively seem that might be important) it's good to start by using a [Support vector regressor](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SV Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, since the focus of this example notebook is on the validation part we will train this model in a very straightforward way, not giving the importance to parameter tuning that we should and using only the test set, to highlight the importance of using a cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVR(kernel='linear')\n",
    "\n",
    "clf.fit(X=X_train, y=y_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVRegressor score is: 0.6168014926864142\n"
     ]
    }
   ],
   "source": [
    "print(\"SVRegressor score is: {}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we obtained roughly 62% of correct predictions using this model. It is not an awful number, but we can do better. Let's then try an Ensemble Regressor, as the cheat-sheet suggests. In this case, we're going to use a [Gradient Boosting Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "             presort='auto', random_state=0, subsample=1.0, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clg = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "clg.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Regressor score is: 0.8465809732594964\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient Boosting Regressor score is: {}\".format(clg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base model was able to achieve roughly 85%, which we can consider a good result. Let's now modify some parameters just to see if the results improve or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Regressor score is: 0.7624149573168991\n"
     ]
    }
   ],
   "source": [
    "clg = GradientBoostingRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "clg.fit(X=X_train, y=y_train)\n",
    "\n",
    "print(\"Gradient Boosting Regressor score is: {}\".format(clg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Regressor score is: 0.8474008501264455\n"
     ]
    }
   ],
   "source": [
    "clg = GradientBoostingRegressor(learning_rate=0.2, random_state=0)\n",
    "\n",
    "clg.fit(X=X_train, y=y_train)\n",
    "\n",
    "print(\"Gradient Boosting Regressor score is: {}\".format(clg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "So in the end we trained four models and validated them on a test set, with the following accuracies:\n",
    "\n",
    "1. Support Vector Regressor: 0.6168\n",
    "2. Gradient Boosting Regressor:  0.8465\n",
    "3. Gradient Boosting Regressor(max_depth=2): 0.7624\n",
    "4. Gradient Boosting Regressor(learning_rate=0.2): 0.8474\n",
    "\n",
    "Based on this simple cross-validation method, the model we would choose was the Gradient Boosting Regressor with the parameter learning rate set to 0.2.\n",
    "\n",
    "Notice that to validate our model we had to set apart 30% of the data we have available. In a world in which we can't ever have enough data to train models, this kind of methods can be costly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a technique called _k-fold_ validation, we will be able to get the same end result without having to lose more samples on the training split. This method's procedure is to split the dataset into _k_ splits, and iteratively use _k-1_ splits to train a model and the remaining split to validate the result. The average of those scores will then be the perfomance measure of the model we have trained. This is done in scikit using the funcion [cross\\_val\\_score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score). In this case we will be using a _5-fold_ validation on the same models we trained on the simple cross-validation section.\n",
    "\n",
    "An auxiliary visualization of the method is the following:\n",
    "\n",
    "![alt text](https://static.oschina.net/uploads/img/201609/26155106_OfXx.png \"5-fold\")\n",
    "<sub>Image Source: https://static.oschina.net/uploads/img/201609/26155106_OfXx.png</sub>\n",
    "\n",
    "After this procedure we can average the scores and also obtain a confidence level on those scores. The method that yields the highest average accuracy will be the chosen one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = SVR(kernel='linear')\n",
    "\n",
    "scores = cross_val_score(clf, boston.data, boston.target, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can then print the scores obtained and also the mean score obtained aswell as the 95% confidence interval of the mean score value we obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of scores is :[ 0.77328953  0.72833447  0.53795481  0.15209389  0.07729196]\n",
      "\n",
      "Accuracy: 0.45 (+/- 0.58)\n"
     ]
    }
   ],
   "source": [
    "print(\"Array of scores is :{}\\n\".format(scores))\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of scores is :[ 0.78572257  0.85705946  0.74913553  0.56263681  0.39369023]\n",
      "\n",
      "Accuracy: 0.67 (+/- 0.34)\n"
     ]
    }
   ],
   "source": [
    "clg = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "scores = cross_val_score(clg, boston.data, boston.target, cv=5)\n",
    "\n",
    "print(\"Array of scores is :{}\\n\".format(scores))\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of scores is :[ 0.8083634   0.88500085  0.74621128  0.53904834  0.51805081]\n",
      "\n",
      "Accuracy: 0.70 (+/- 0.29)\n"
     ]
    }
   ],
   "source": [
    "clg = GradientBoostingRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "scores = cross_val_score(clg, boston.data, boston.target, cv=5)\n",
    "\n",
    "print(\"Array of scores is :{}\\n\".format(scores))\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of scores is :[ 0.78911916  0.79977633  0.72215222  0.60049527  0.41089731]\n",
      "\n",
      "Accuracy: 0.66 (+/- 0.29)\n"
     ]
    }
   ],
   "source": [
    "clg = GradientBoostingRegressor(learning_rate=0.2, random_state=0)\n",
    "\n",
    "scores = cross_val_score(clg, boston.data, boston.target, cv=5)\n",
    "\n",
    "print(\"Array of scores is :{}\\n\".format(scores))\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "As we can see the scenario is very different right now. Looking at individual scores is easy to verify that sometimes the test set influences a lot your final accuracy. This means that a method such as _k-fold_ cross-validation is much more robust to this kind of possibilities.\n",
    "\n",
    "Let's list all the results and check which one did best:\n",
    "\n",
    "1. Support Vector Regressor: 0.45 (+/- 0.58)\n",
    "2. Gradient Boosting Regressor:  0.67 (+/- 0.34)\n",
    "3. Gradient Boosting Regressor(max_depth=2): 0.70 (+/- 0.29)\n",
    "4. Gradient Boosting Regressor(learning_rate=0.2): 0.66 (+/- 0.29)\n",
    "\n",
    "According to _k-fold_ cross-validation, a Gradient Boosting Regressor with parameter max_depth set to 2, is the best model (highest score with lowest uncertainty) from the ones we tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another strategy to use cross-validation is to use random splits instead of fixed splits with function [ShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html). When using random splits (in which the data is randomly chosen from the dataset everytime you \"create\" a fold) it is not guaranteed that each fold will be different, even though this can be assumed if the dataset is big enough.\n",
    "\n",
    "Let's see how this influences the results on the models we have been trying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of scores is :[ 0.78569463  0.59163902  0.74280836  0.69801539  0.64550047]\n",
      "\n",
      "Accuracy: 0.69 (+/- 0.14)\n"
     ]
    }
   ],
   "source": [
    "clf = SVR(kernel='linear')\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=1)\n",
    "\n",
    "scores = cross_val_score(clf, boston.data, boston.target, cv=cv)\n",
    "\n",
    "print(\"Array of scores is :{}\\n\".format(scores))\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of scores is :[ 0.92216968  0.8710234   0.91212971  0.89990485  0.81722107]\n",
      "\n",
      "Accuracy: 0.88 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "clg = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=1)\n",
    "\n",
    "scores = cross_val_score(clg, boston.data, boston.target, cv=cv)\n",
    "\n",
    "print(\"Array of scores is :{}\\n\".format(scores))\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of scores is :[ 0.90274258  0.84053833  0.90234705  0.8669969   0.84641735]\n",
      "\n",
      "Accuracy: 0.87 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "clg = GradientBoostingRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=1)\n",
    "\n",
    "scores = cross_val_score(clg, boston.data, boston.target, cv=cv)\n",
    "\n",
    "print(\"Array of scores is :{}\\n\".format(scores))\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of scores is :[ 0.92269094  0.88621864  0.91655039  0.89897041  0.82202122]\n",
      "\n",
      "Accuracy: 0.89 (+/- 0.07)\n"
     ]
    }
   ],
   "source": [
    "clg = GradientBoostingRegressor(learning_rate=0.2, random_state=0)\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=1)\n",
    "\n",
    "scores = cross_val_score(clg, boston.data, boston.target, cv=cv)\n",
    "\n",
    "print(\"Array of scores is :{}\\n\".format(scores))\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score results obtained for the Gradient Boosting Regressor are very high compared with the fixed split. This may mean that the data has some fixed portions in which the model does not work well and those portions keep lowering the scores we obtain. Shuffling the data allows us to have a more varied distribution of samples in each fold and it allows each of them to resemble more to the dataset as an whole.\n",
    "\n",
    "In this case the results are similar for the three models tested and any of them would be a good choice according to this values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training and validating a model it is important to make sure that the same preprocessing and data transformations are done in both training and test/validation data. In scikit, the [pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) function is an easy way to take care of this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.846708961938753\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.3, random_state=0)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "\n",
    "clg = GradientBoostingRegressor(random_state=0).fit(X_train_transformed, y_train)\n",
    "\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(clg.score(X_test_transformed, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to extend this to a _k-fold_ cross-validation method, instead of doing this \"manually\" for each of the possibilities, we can use [_make\\_pipeline_](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) and easily extend the preprocessing to all the tests being done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of scores is :[ 0.92269094  0.88621864  0.91655039  0.89897041  0.82202122]\n",
      "\n",
      "Accuracy: 0.89 (+/- 0.07)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "clg = make_pipeline(preprocessing.StandardScaler(), GradientBoostingRegressor(random_state=0))\n",
    "cross_val_score(clg, boston.data, boston.target, cv=5)\n",
    "\n",
    "print(\"Array of scores is :{}\\n\".format(scores))\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
