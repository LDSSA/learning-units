{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Validation Metrics for Classification\n",
    "\n",
    "Scikit-learn covers extensively the classification validation metrics [[here](http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)]. The ones presented here are:\n",
    "- Accuracy Score\n",
    "- Confusion Matrix\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "- ROC and AUROC\n",
    "\n",
    "By: Hugo Lopes  \n",
    "Learning Unit 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, \\\n",
    "    recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load some example scores (output of a classifier)\n",
    "The example data (from binary classification), presented next, contains:\n",
    "- Column 1: **`scores`** or *probas* (output of `predict_proba()`) in the range [0, 1]\n",
    "- Column 2: **`target`** or actual outcome (y truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv('../data/classifier_prediction_scores.csv')\n",
    "print('Number of rows:', df_results.shape[0])\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the scores distribution. As an output of the `predict_proba()`, the scores range is [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['scores'].hist(bins=50)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Scores')\n",
    "plt.title('Distribution of Scores')\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics\n",
    "## Accuracy score\n",
    "The [accuracy_score](http://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score) is the fraction (default) or the count (normalize=False) of correct predictions. It is given by:  \n",
    "\n",
    "$$ A = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "Where, TP is the True Positives, TN the True Negatives, FP the False Positives, and False Negative.\n",
    "\n",
    "Disavantages:\n",
    "- Not recommended its use in highly imbalanced datasets.\n",
    "- You have to set a threshold for the output of the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the threshold above which the predicted label is considered 1:\n",
    "threshold = 0.50\n",
    "# Generate the predicted labels (above threshold = 1, below = 0)\n",
    "predicted_outcome = [0 if k <= threshold else 1 for k in df_results['scores']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy = %2.3f' % accuracy_score(df_results['target'], predicted_outcome))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "The [confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) *C* provides several performance indicators:\n",
    "- C(0,0) - TN count\n",
    "- C(1,0) - FN count\n",
    "- C(0,1) - FP count\n",
    "- C(1,1) - TP count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the confusion matrix:\n",
    "confmat = confusion_matrix(y_true=df_results['target'], y_pred=predicted_outcome)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.4)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j, y=i,\n",
    "        s=confmat[i, j],\n",
    "        va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the number of False Negatives is very high, which, depending on the business could be harmful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall and F1-score\n",
    "- [**Precision**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score) is the ability of the classifier not to label as positive a sample that is negative (i.e., a measure of result relevancy).\n",
    "$$ P = \\frac{T_P}{T_P+F_P} $$  \n",
    "  \n",
    "  \n",
    "- [**Recall**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score) is the ability of the classifier to find all the positive samples (i.e., a measure of how many truly relevant results are returned).\n",
    "$$ R = \\frac{T_P}{T_P+F_N} $$  \n",
    "  \n",
    "  \n",
    "- [**F1 score**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) can be interpreted as a weighted harmonic mean of the precision and recall (in this case recall and precision are equally important).\n",
    "$$ P = 2\\frac{P \\times R}{P+R} $$\n",
    "\n",
    "where $T_P$ is the true positives, $F_P$ the false positives, and $F_N$ the false negatives. Further information on [precision, recall and f1-score.](http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-and-f-measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check if our dataset has **class imbalance**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather imbalanced! Approximately 83% of the labels are 0. Let's take a look at the other metrics more appropriate for this type of datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision score = %1.3f' % precision_score(df_results['target'], predicted_outcome))\n",
    "print('Recall score = %1.3f' % recall_score(df_results['target'], predicted_outcome))\n",
    "print('F1 score = %1.3f' % f1_score(df_results['target'], predicted_outcome))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the results actually not so good as the accuracy metric would show us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic (ROC) and Area Under the ROC (AUROC)\n",
    "\n",
    "The ROC is very common for binary classification problems. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings.  \n",
    "- The [**`roc_curve`**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve) compute Receiver operating characteristic (ROC)\n",
    "- The [**`roc_auc_score`**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) function computes the area under the receiver operating characteristic (ROC) curve. The curve information is summarized in one number.  \n",
    "\n",
    "Unlike the previous metrics, the ROC functions above require the actual scores/probabilities (and not the predicted labels). Further information on [roc_curve and roc_auc_score](http://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc). This metric is rather useful for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to compute the ROC curve (FPR and TPR):\n",
    "fpr, tpr, thresholds = roc_curve(df_results['target'], df_results['scores'])\n",
    "# The Area Under the ROC curve:\n",
    "roc_auc = roc_auc_score(df_results['target'], df_results['scores'])\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8,6))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='orange', lw=lw, label='ROC curve (AUROC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--', label='random')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.grid()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the AUROC is 0.70. A value of 0.50 means that the classifier is no better than random."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
