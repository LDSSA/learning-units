# Validation metrics

At this point it is supposed that you fitted a classifier and have the 
outputs of the prediction in terms of ranks (usually the output of 
`clf.predict_proba()`).  

#### [Presentation](https://docs.google.com/presentation/d/1oVtD4YBW_uNY5R0pLqDa-zi0-TSqO4JlcUsRkCZp7s0/edit?usp=sharing)

# New concepts in this unit
- Validation metrics for classification:
  - Accuracy
  - Precision & Recall
  - F1-score
  - ROC and ROC AUC
- Class Imbalance

# New tools in this unit


# Exercise
Open the titanic_exercise.csv dataset (in the data folder), and attempt to 
solve the problems detected in the previous unit.
You should be able to act on the following:
* Evaluate the performance of your classifier in terms of the above 
validation metrics.